MNIST Report

So I chose my hyper-parameters based on what was written in Tensorflow's tutorial.  
From the original MNISTStarter.py, I added two hidden layers to the model (a dense 'relu' layer, and a dropout layer), and changed the output layer's activation function from sigmoid to softmax. The hidden layers are both from the tutorial, but I increased the size of the dense layer from 128 to 200, and decreased the dropout amount from 20% to 10% (i.e. randomly drops 20 input tensors). 
I changed the model optimizer from sgd to adam (like in the tutorial). I tried using a few of the other included optimizers, but adam seemed to produce the most consistant results.
The loss function remained the same (sparse categorical cross-entropy).
I played around with using various epoch amounts, and found anything past 4 produced similar results; since the training doesn't take too long, I increased the epochs to 8 (because why not, right?).
All-in-all, most of what I did was tweak the parameters until I got a model with 99%/ 98% training/ test accuracy.

Hyper-Parameters:

Model: Sequential (4 layers)
1: Input layer (size = 784)
2: Dense layer (size = 200, activation = relu)
3: Dropout layer (rate = 0.1)
4: Output layer (size = 10, activation = softmax)

Optimizer: adam (stochastic gradient descent)
Loss: sparse categorical cross-entropy

Epochs: 8

Results:

    Epoch 1/8
    60000/60000 - 3s - loss: 0.2516 - accuracy: 0.9270
    Epoch 2/8
    60000/60000 - 3s - loss: 0.1114 - accuracy: 0.9668
    Epoch 3/8
    60000/60000 - 3s - loss: 0.0780 - accuracy: 0.9755
    Epoch 4/8
    60000/60000 - 3s - loss: 0.0598 - accuracy: 0.9804
    Epoch 5/8
    60000/60000 - 3s - loss: 0.0481 - accuracy: 0.9845
    Epoch 6/8
    60000/60000 - 3s - loss: 0.0384 - accuracy: 0.9878
    Epoch 7/8
    60000/60000 - 3s - loss: 0.0311 - accuracy: 0.9898
    Epoch 8/8
    60000/60000 - 3s - loss: 0.0277 - accuracy: 0.9907
    --Evaluate model--
    10000/1 - 0s - loss: 0.0326 - accuracy: 0.9810
    Model Loss:    0.06
    Model Accuracy: 98.1%